{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neemiasbsilva/MLLMs-Teoria-e-Pratica/blob/main/use-cases/IdentifyFeatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_N0W4eKLd6B"
      },
      "source": [
        "# DeepSeek VL - Identificação de objetos\n",
        "\n",
        "Bem vindos a esse notebook com um pequeno tutorial para identificação de objetos em imagens utilizando uma LLM multimodal!\n",
        "\n",
        "É muito importante que cada célula seja executada em sequência, conforme será explicado em logo mais.\n",
        "\n",
        "Neste tutorial, devido às limitações do Google Colab, vamos utilizar o [Deepseek-VL](https://github.com/deepseek-ai/DeepSeek-VL). Então, Vamos começar clonando seu repositório:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvh5d9CFTq2T"
      },
      "source": [
        "## Configuração do Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGO0KbSE_xTx",
        "outputId": "1ea8f9cf-ca56-43c6-d33f-c3dfe2d4512a"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/deepseek-ai/DeepSeek-VL -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_4UyL7BoAgx"
      },
      "source": [
        "Em seguida, vamos instalar as dependências, tomando cuidado para instalar o Torch separado, já que este é sensível a versão que estamos utilizando de Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cylUcvr-AKGn"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q\n",
        "!sed -i '/torch==/d' /content/DeepSeek-VL/requirements.txt\n",
        "!pip install -r /content/DeepSeek-VL/requirements.txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2EmE48GU4Os"
      },
      "source": [
        "Após a instalação precisa reinicar a execução para que os pacotes necessários sejam devidamente instalados no kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuMNKtpST3oB"
      },
      "source": [
        "## Carregar o Modelo e Inicialização do Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Nryw80pTC-"
      },
      "source": [
        "Agora, vamos utilizar os métodos da biblioteca transformers e o torch para carregar o modelo na nossa placa gráfica, inicializar nosso tokenizer, que será responsável pela codificação das entradas, e decodificação das saídas para o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "870652d3cfc349719c35b8f925aa3595",
            "8018efeece80497ab5529f4f3789423e",
            "e485c7a763b448f989d7392937f2ead8",
            "87150c9478844b959f9e4b7b6ac47170",
            "f67defd87b7f48cfa24d8d2bf79037df",
            "407994cf0c3540139921ce9d2c1275e9",
            "40a9e8b11b63499489b509966920a154",
            "ae200d3706884055a754b25643872bbf",
            "29f8e49f9f314317bf49c8ce33d1d451",
            "b0f44b11b5c0450f8580726d0617dd53",
            "c79a8e3b9da647168353e3d757e6779b",
            "e217b228baf14bd28c4f685b02a62272",
            "283cad20c3704823898609df982e767c",
            "7b7ae8d0342740f89576a07f5be91a77",
            "649795a556c4400484f67b79bbfa0599",
            "0a1a7d6b4aa148cb9264d05b71e439dc",
            "3d18a5c51900459484bb1b9aa71fff3e",
            "d2f3024e90554b9ebe572ed30bc098da",
            "631d5b8dfad24217a2a7c49256514813",
            "92d918dc25e64911bee136ce8f0b6375",
            "6f8300f18d72461b9c5d7e68b83a7b87",
            "c8e614752fe24994baadc02f1482a20f",
            "988314e62cbf428bbbad49341b80d75e",
            "c56014eb4c694aa1bef8dd785da30952",
            "af8dd66f189f4dc59aaaacedb91a100c",
            "240bb514c3e647bf88e9b418244732b4",
            "458d729b797b486688cd3461f4cc2930",
            "7faac8b8306f47dd8920bfbff52e5fb6",
            "a42ce305f3f8491dbc02001f366114da",
            "17bea4031a8643ebac60ad96644d0cde",
            "c706f44d2b1641ffbb7d8d12f67877aa",
            "68d519eeeea44e87a05aef38189e7a00",
            "dfd18a95bdd84101833e3ff1237ac284",
            "3ae69625daa248a59806421fa470b796",
            "32dc6a1ea376417aaf489ecdcbda87d1",
            "cfd43048885b44c0a2e8f07b27229e2b",
            "23b4ac41b76f4bd2a732fda47d86f3ed",
            "ef688ac9c10041618ef3bb144fb3d956",
            "adf846ed198d43b7b44af374e6e8d3ea",
            "e69bd279d9e64bf5965a646c77e36b8a",
            "1a0ff54899f946a09c804235378d4c3a",
            "6b3e0af2ee36447bb109a57b59ce3029",
            "fa78b0751deb40a9aee082eafd017736",
            "f9ebd5a925e44d4a940ec12bb8cace9b",
            "a1410aa11ad04a648370de2420f9d66a",
            "ce931fba15b1449e8a2fd953f717e797",
            "0d58b939fcc247d38891a447aab4010b",
            "7b2e9df574a84ce2be3e9fb37306ec77",
            "6cdb07fb18b7407fa2128dbd1b2c4b0d",
            "23cb86af004a49f39bda4a5bb2b61c0b",
            "d35c915f80b74d46aaef8e9bf3f48b4e",
            "2fce2bf3258a48f796e0cfb97907df4e",
            "91fea8fc65fe4d18a632edd9c97497f5",
            "ccdc3263dd56415590488e7baf844d22",
            "135db5e8550c4d1c8e47cf0963dbc716",
            "f4c99869fbaf45bd9025482ac5c525ba",
            "f9236ec1f7354504bf2e2f3a3175bcff",
            "89e05e4d1d3e411086eaf97dd51041ee",
            "c9396d6a131d493d95edc39bceba6ab7",
            "cf7529735e464f3ea26c83faa52bb70f",
            "362107e527f0453090bced6d5ef6a130",
            "2398171be97847c3b42f93535b738aac",
            "a1026304db444b448734ae21e556471d",
            "ee3cce642585481a96576833f8832988",
            "1146713620c643aeb711fca5767b62b9",
            "da203fbf68b64a9d84a234e2908918b4",
            "ef4ae6b3256d431598596b20aa90ae7b",
            "724d5a9238db41179a9dc8eef992cc6f",
            "d263223cf4314adab48af0ca14b9d03b",
            "81383c307b414f4f9c9c5a0c49f7a617",
            "49b1f9635f994728b32a39a7f1253784",
            "077395f2b2874e3d955e1c0549cca5e4",
            "721ff80716c74df1981158e56dc8beae",
            "0e894ac0e2e8424db911119a5ca8a3bc",
            "6ccfb1f0baf64d978ce30b829d3b3aa6",
            "947e3d718dc64b6abd4dad8923646e18",
            "afa46454df144ce9839da7c8e8159d82"
          ]
        },
        "id": "o3tsDPkCA-eO",
        "outputId": "1dbd5234-a524-4ae7-b26d-a62f359dd232"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sys.path.append('/content/DeepSeek-VL')\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
        "\n",
        "model_path = \"deepseek-ai/deepseek-vl-1.3b-base\"\n",
        "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
        "tokenizer = vl_chat_processor.tokenizer\n",
        "\n",
        "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
        "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGbxbLdcF1d_"
      },
      "source": [
        "## Carregando o Modelo e Fazendo um Teste de Infêrencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okjkjkPXpsnO"
      },
      "source": [
        "Vamos rodar este teste para verificar se o modelo está carregado corretamente! Note a reposta sendo mostrada no terminal, e todo o pré-processamento que um modelo multimodal exige, como criação dos embeddings das imagens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "_PMqz8iGDZap",
        "outputId": "a88dc873-caca-4f35-c692-4e49c3dceb3c"
      },
      "outputs": [],
      "source": [
        "from deepseek_vl.utils.io import load_pil_images\n",
        "from IPython.display import Image, display\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"User\",\n",
        "        \"content\": \"\"\"<image_placeholder>Analyze the image and identify which of the features you are certain are in this list and in the image: [dog, cat, rat, bird, sheep]. Do not mention features not in the list.\n",
        "        \"\"\",\n",
        "        \"images\": [\"/content/DeepSeek-VL/images/dog_a.png\"]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"Assistant\",\n",
        "        \"content\": \"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# load images and prepare for inputs\n",
        "pil_images = load_pil_images(conversation)\n",
        "prepare_inputs = vl_chat_processor(\n",
        "    conversations=conversation,\n",
        "    images=pil_images,\n",
        "    force_batchify=True\n",
        ").to(vl_gpt.device)\n",
        "\n",
        "# run image encoder to get the image embeddings\n",
        "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
        "\n",
        "# run the model to get the response\n",
        "outputs = vl_gpt.language_model.generate(\n",
        "    inputs_embeds=inputs_embeds,\n",
        "    attention_mask=prepare_inputs.attention_mask,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "display(Image(\"/content/DeepSeek-VL/images/dog_a.png\"))\n",
        "\n",
        "answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "print(f\"{prepare_inputs['sft_format'][0]}\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaEdDjqEF9B6"
      },
      "source": [
        "## Inferência em Batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE9NVDOTqEWt"
      },
      "source": [
        "Caso dê tudo certo, vamos agora tentar um processamento em batch.\n",
        "\n",
        "Como já foi detalhado no material, vamos focar na task 1. então focamos puramente em passar a imagem, e um prompt para o modelo.\n",
        "\n",
        "Neste exemplo, utilizamos o subconjunto alpha5 p3, onde todos os 5 examinadores concordaram no sentimento de uma imagem, com três classes.\n",
        "\n",
        "Mais detalhes, e acesso para essa base de dados completa podem ser acessados [aqui](https://arxiv.org/abs/2508.16873)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ9X5Ua3JtkU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "base_url = \"https://drive.usercontent.google.com/download?export=view&id=\"\n",
        "ids = [\"1mKb_EHSAgsOmxJsTFleVUvtGv4Uj8KQ0\", \"1UHIfdHUFkOsamfVcL_QwAG7TFc_gNwqk\", \"1QcaUZ0Ac-XsUmZvv-sIZr8x1waK7gnwL\", \"1o3a37CSBkb8uuPbg7xEItMc-47ZksmNl\", \"1_cDlBPgjCkokLs3xSLihxAys6WuWIDnW\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsiTxmW1rpTp"
      },
      "source": [
        "Depois de carregado nosso conjunto, vamos passar por cada uma das imagens, passando para o modelo a imagem, e o nosso prompt, propriamente preparado, e mostrar a reposta dele! Note que para esse modelo, extrair o sentimento exato da resposta não é uma tarefa simples, mas em modelos mais avançados, essa tarefa se torna um pouco mais fácil, já que tendem a dar uma resposta mais assertiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QYdoB8LxiPF8",
        "outputId": "11ca55db-a0d2-4c60-8d01-c7d67a5f504c"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import base64\n",
        "\n",
        "for id in ids:\n",
        "  image_url = base_url + id\n",
        "  response = requests.get(image_url)\n",
        "  b64response = uri = (\"data:\" +\n",
        "                response.headers['Content-Type'] + \";\" +\n",
        "                \"base64,\" + str(base64.b64encode(response.content).decode(\"utf-8\")))\n",
        "\n",
        "  conversation = [\n",
        "    {\n",
        "        \"role\": \"User\",\n",
        "        \"content\": \"\"\"<image_placeholder>Analyze the image and provide a list of features that you are certain are present in the image. Ignore features not in the provided list.\n",
        "\n",
        "        PROVIDED LIST:\n",
        "        [sidewalk, tree-lined street, porch, fenced front yard, attached garage, cul-de-sac, hills, private front entrance]\n",
        "\n",
        "        EXAMPLE OUTPUT:\n",
        "        [sidewalk, porch, attached garage]\n",
        "        \"\"\",\n",
        "        \"images\": [b64response]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"Assistant\",\n",
        "        \"content\": \"\"\n",
        "    }\n",
        "  ]\n",
        "\n",
        "  pil_images = load_pil_images(conversation)\n",
        "  prepare_inputs = vl_chat_processor(\n",
        "      conversations=conversation,\n",
        "      images=pil_images,\n",
        "      force_batchify=True\n",
        "  ).to(vl_gpt.device)\n",
        "\n",
        "  inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
        "\n",
        "  outputs = vl_gpt.language_model.generate(\n",
        "    inputs_embeds=inputs_embeds,\n",
        "    attention_mask=prepare_inputs.attention_mask,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    use_cache=True\n",
        "  )\n",
        "\n",
        "  answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "  display(Image(data = response.content))\n",
        "  print('--- id: ' + id + '---')\n",
        "  print(f\"{prepare_inputs['sft_format'][0]}\", answer)\n",
        "  print('---------')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHN6KRaHjc64"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

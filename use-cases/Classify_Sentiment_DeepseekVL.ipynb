{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neemiasbsilva/MLLMs-Teoria-e-Pratica/blob/main/use-cases/Classify_Sentiment_DeepseekVL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_N0W4eKLd6B"
      },
      "source": [
        "# DeepSeek VL - Classificação de Sentimento\n",
        "\n",
        "Bem vindos a esse notebook com um pequeno tutorial para inferência em uma LLM multimodal, com um pedacinho de interface com pandas!\n",
        "\n",
        "É muito importante que cada célula seja executada em sequência, conforme será explicado em logo mais.\n",
        "\n",
        "Neste tutorial, devido às limitações do google Collab, vamos utilizar o [Deepseek-VL](https://github.com/deepseek-ai/DeepSeek-VL). Então, Vamos começar clonando seu repositório:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvh5d9CFTq2T"
      },
      "source": [
        "## Configuração do Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGO0KbSE_xTx",
        "outputId": "c8851a09-f252-4239-d0d2-7b682c86de52"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/deepseek-ai/DeepSeek-VL -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_4UyL7BoAgx"
      },
      "source": [
        "Em seguida, vamos instalar as dependências, tomando cuidado para instalar o Torch separado, já que este é sensível a versão que estamos utilizando de Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cylUcvr-AKGn"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q\n",
        "!sed -i '/torch==/d' /content/DeepSeek-VL/requirements.txt\n",
        "!pip install -r /content/DeepSeek-VL/requirements.txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2EmE48GU4Os"
      },
      "source": [
        "Após a instalação precisa reinicar a execução para que os pacotes necessários sejam devidamente instalados no kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuMNKtpST3oB"
      },
      "source": [
        "## Carregar o Modelo e Inicialização do Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Nryw80pTC-"
      },
      "source": [
        "Agora, vamos utilizar os métodos da biblioteca transformers e o torch para carregar o modelo na nossa placa gráfica, inicializar nosso tokenizer, que será responsável pela codificação das entradas, e decodificação das saídas para o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311,
          "referenced_widgets": [
            "801c35fd912a4eaeb12012878ce836df",
            "e5014b36e56a436ca53dcba9157226f5",
            "42c7d6f85949405babdddc10a365d760",
            "e8180fdd055b45d4959fb3642c1301cc",
            "460fd5db27934bfeb9c70c1b9e4a81fa",
            "9cf9fd66a4ea4e9c9e3e7b453c400be9",
            "36e748306645495a97e2c66ca1839ca6",
            "b1b30e2e166749f1ad78aaf34cc942d8",
            "5326c751839c4663900cf47441a06b2a",
            "c82739ff737b407d8c313828dc2fc895",
            "1983374baae447348c7b9505124d9d1b",
            "edfdec5ee548487fb21c3b2c1fdc7225",
            "0cc5d910c79d427fbeb3b163b07b39a7",
            "9948149cea3b44e9be8115f73b080774",
            "2fcedf1465fa4c4bb6eb23b67299d57c",
            "3a8ddf9d7888413d9ddd5741f76f9907",
            "05b4e37fb0594ba4b020010544db1973",
            "71e7eefb67974b42bac2d12ca1e2f8d0",
            "b6eff76550a04a86ac639e585cf072e9",
            "c7133aeb5084406d9f4e097ebd9c54e3",
            "f4f1d10aba8d4075a8d327a25409fe85",
            "eb86c55c7469448193ed3b3ea7a3f01f",
            "1edcc0bb290f48cdadd347f6c9f3ee50",
            "bb230c471f304f248994ed52332ffd2b",
            "0b91088afad6493284f21d6e543598d7",
            "37666dbd4c504ec8b0d19f8b386f0de7",
            "9481714aa2274c6c93693c89961eb037",
            "7d1305e7a56649c6a016f61e665a7874",
            "9d7917640c094042985dce3beb6c0bcf",
            "4559a677370c4ccc8b9f17b652170eca",
            "61997dc4b406445296415be2a0b0c530",
            "cad1f018f9ae49949a9ff27483e7cda4",
            "f27b6a47acc641a28870d94347d6b9af",
            "f6a5914d4d824ec490e3c4b5f58cd0ee",
            "bc1af29b07314dcf92f0ffcee76c3a7e",
            "f52805a3c4eb43f3b59a2d386a755760",
            "f535375109d74444b9e80d51bd0a63cc",
            "6e51168840384cc6ac062e6505009267",
            "cd63c3dfc05a4e1e8fe0cbe91f82cd6a",
            "fa1468f5c7b04df6adb7ece414166c90",
            "5782870efef74586b4b1bea7d7bab081",
            "7400ad8a2fea4d49ad65c258982f8f66",
            "8b7fb357c1a143cfaca1679cfb0f4660",
            "4df1c6fe8a47433eb098a88e82d654e2",
            "b0bb31867c8645449ae0929817d32b35",
            "9d4310f4902444aa986f0fb9799543f1",
            "f0e023d957084014bb3f8c293bdc67f9",
            "6648aca136c04efab717042a1b84c0fd",
            "bdc21119442b4cc2939c1ba49e3ef440",
            "f3b5c47e7ffa4445bb31db7a92272cea",
            "4e5bcc39c2084b1fb45f3d676dd17c56",
            "dcd9024b444a4125bfc4a2a8c43491f9",
            "873dd1698599479985eb5b107febd613",
            "166f8f2506a848768789633c7f4939fe",
            "6d4a71546338410483bdb0ba2c8a07a4",
            "09a88924c3604606a359857321d52da7",
            "d9306f6290264302b48ea44440a433cd",
            "e989a75388bc4104af9154ec18a8c98d",
            "175013833d9e48f695a788daeef5ed85",
            "cb45a87a2d884c25a72ff35ce123a38f",
            "378db3f3d78346c9b41d4f0faedb0367",
            "f5f7fe49bbeb4d58adb27aec46b7edbf",
            "bd667d70a8ff4e45a72e9d272d6abd4c",
            "98ffb7af31294daeb2e201ee7f2a1c55",
            "128489d605214b8d9a4273ad7e5ffcfa",
            "402ca6d89e3743209b79b1e36a30e1ad",
            "bdbcea669a8a4118a7cfb4f7494bd81c",
            "c90a193174a341f2bba8c8f303690974",
            "763b7460596d43fca4a0a8eb7346f3d9",
            "fba361a7fb9b4a8abf275cf48df9df13",
            "f30ed2c2e957466788c2c4a215d44e9e",
            "500dcdcd6b1e4f35b063b0288bd78b65",
            "6bc73d786a48468fb7b1032b968482f0",
            "a6ea67b3a67d4a9dbc08061b7acfd8cf",
            "0622ec56a68c476682d156407fc36c59",
            "f5ff01d0d4a14ea3b47c1abfaef1f92d",
            "ff374eab74e948ab9a43ced7de9254ba"
          ]
        },
        "id": "o3tsDPkCA-eO",
        "outputId": "81a2bca8-9858-47d4-bac0-c385d1965de4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sys.path.append('/content/DeepSeek-VL')\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
        "\n",
        "model_path = \"deepseek-ai/deepseek-vl-1.3b-base\"\n",
        "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
        "tokenizer = vl_chat_processor.tokenizer\n",
        "\n",
        "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
        "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGbxbLdcF1d_"
      },
      "source": [
        "## Carregando o Modelo e Fazendo um Teste de Infêrencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okjkjkPXpsnO"
      },
      "source": [
        "Vamos rodar este teste para verificar se o modelo está carregado corretamente! Note a reposta sendo mostrada no terminal, e todo o pré-processamento que um modelo multimodal exige, como criação dos embeddings das imagens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PMqz8iGDZap",
        "outputId": "9300a849-5aa2-4947-fb07-9f888dd2ce4a"
      },
      "outputs": [],
      "source": [
        "from deepseek_vl.utils.io import load_pil_images\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"User\",\n",
        "        \"content\": \"<image_placeholder> Analyze this image, and describe it, analysing its sentiment.\",\n",
        "        \"images\": [\"/content/DeepSeek-VL/images/dog_a.png\"]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"Assistant\",\n",
        "        \"content\": \"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# load images and prepare for inputs\n",
        "pil_images = load_pil_images(conversation)\n",
        "prepare_inputs = vl_chat_processor(\n",
        "    conversations=conversation,\n",
        "    images=pil_images,\n",
        "    force_batchify=True\n",
        ").to(vl_gpt.device)\n",
        "\n",
        "# run image encoder to get the image embeddings\n",
        "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
        "\n",
        "# run the model to get the response\n",
        "outputs = vl_gpt.language_model.generate(\n",
        "    inputs_embeds=inputs_embeds,\n",
        "    attention_mask=prepare_inputs.attention_mask,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "print(f\"{prepare_inputs['sft_format'][0]}\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaEdDjqEF9B6"
      },
      "source": [
        "## Inferência em Batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE9NVDOTqEWt"
      },
      "source": [
        "Caso dê tudo certo, vamos agora tentar um processamento em batch.\n",
        "\n",
        "Como já foi detalhado no material, vamos focar na task 1. então focamos puramente em passar a imagem, e um prompt para o modelo.\n",
        "\n",
        "Neste exemplo, utilizamos o subconjunto alpha5 p3, onde todos os 5 examinadores concordaram no sentimento de uma imagem, com três classes.\n",
        "\n",
        "Mais detalhes, e acesso para essa base de dados completa podem ser acessados [aqui](https://arxiv.org/abs/2508.16873)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sZ9X5Ua3JtkU",
        "outputId": "feca7211-f4b9-4c10-931b-9d546d451a26"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://drive.google.com/uc?export=download&id=1OLFvsOI_YPi4HxGBqjmI68uqw4dmmNT9')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsiTxmW1rpTp"
      },
      "source": [
        "Depois de carregado nosso conjunto, vamos passar por cada uma das imagens, passando para o modelo a imagem, e o nosso prompt, propriamente preparado, e mostrar a reposta dele! Note que para esse modelo, extrair o sentimento exato da resposta não é uma tarefa simples, mas em modelos mais avançados, essa tarefa se torna um pouco mais fácil, já que tendem a dar uma resposta mais assertiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "QYdoB8LxiPF8",
        "outputId": "9baf82a5-1105-4679-8ffe-3d6cb3e6407a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import base64\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  image_url = row[\"url\"]\n",
        "  response = requests.get(image_url)\n",
        "  b64response = uri = (\"data:\" +\n",
        "                response.headers['Content-Type'] + \";\" +\n",
        "                \"base64,\" + str(base64.b64encode(response.content).decode(\"utf-8\")))\n",
        "\n",
        "  conversation = [\n",
        "    {\n",
        "        \"role\": \"User\",\n",
        "        \"content\": \"<image_placeholder>Analyze this image, and describe it, analysing its sentiment.\",\n",
        "        \"images\": [b64response]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"Assistant\",\n",
        "        \"content\": \"\"\n",
        "    }\n",
        "  ]\n",
        "\n",
        "  pil_images = load_pil_images(conversation)\n",
        "  prepare_inputs = vl_chat_processor(\n",
        "      conversations=conversation,\n",
        "      images=pil_images,\n",
        "      force_batchify=True\n",
        "  ).to(vl_gpt.device)\n",
        "\n",
        "  inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
        "\n",
        "  outputs = vl_gpt.language_model.generate(\n",
        "    inputs_embeds=inputs_embeds,\n",
        "    attention_mask=prepare_inputs.attention_mask,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    use_cache=True\n",
        "  )\n",
        "\n",
        "  answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "  print(f\"{prepare_inputs['sft_format'][0]}\", answer)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHN6KRaHjc64"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

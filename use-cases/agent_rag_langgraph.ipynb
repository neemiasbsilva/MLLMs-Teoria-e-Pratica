{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLalYD48dDTq6qEqljvhni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neemiasbsilva/MLLMs-Teoria-e-Pratica/blob/main/use-cases/agent_rag_langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hands-On: Construindo um Agent RAG com LangGraph\n",
        "\n",
        "**Objetivo**: Criar um agente de Retrieval-Augmented Generation (RAG) do zero usando `langgraph`. Este agente será capaz de responder perguntas com base em um documento da web, e o mais importante, ele saberá \"decidir\" o que fazer se não encontrar a informação.\n",
        "\n",
        "Importante: Para rodar este Colab, você precisa de uma API Key do Google AI Studio.\n",
        "\n",
        " - Acesse [aistudio.google.com/app/apikey](aistudio.google.com/app/apikey)\n",
        "\n",
        " - Clique em \"Create API key\" e copie a chave.\n",
        "\n",
        " - No Colab, clique no ícone de \"Chave\" na barra lateral esquerda, crie um novo \"secret\" chamado GOOGLE_API_KEY e cole sua chave lá."
      ],
      "metadata": {
        "id": "LOFWmCd1PBrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalação e Configuração\n",
        "\n",
        "Primeiro, vamos instalar todas as bibliotecas necessárias e configurar nossa chave de API do Google Gemini."
      ],
      "metadata": {
        "id": "pJDuzLjGPbUH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t9-6lI4NhFJ"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langgraph langchain langchain_google_genai langchain_community langchain_huggingface faiss-cpu sentence-transformers beautifulsoup4 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "# Tenta carregar a chave do Colab Secrets\n",
        "try:\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "except:\n",
        "    # Se não funcionar, pede para o usuário digitar\n",
        "    print(\"Chave da API não encontrada no Colab Secrets.\")\n",
        "    print(\"Cole sua GOOGLE_API_KEY abaixo e pressione Enter:\")\n",
        "    os.environ['GOOGLE_API_KEY'] = getpass.getpass()\n",
        "\n",
        "if not os.environ.get('GOOGLE_API_KEY'):\n",
        "    raise Exception(\"API Key do Google não configurada. O notebook não pode continuar.\")\n",
        "\n",
        "print(\"API Key configurada com sucesso!\")"
      ],
      "metadata": {
        "id": "Q7oi-8oJPm01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparando os Dados (o \"R\" do RAG)\n",
        "\n",
        "Nosso agente precisa de uma base de conhecimento. Vamos usar uma página web como nossa \"memória externa\".\n",
        "\n",
        "1. Carregar: Baixar o conteúdo de uma página web.\n",
        "2. Dividir: Quebrar o texto em pedaços (chunks).\n",
        "3. Embarcar: Transformar cada pedaço em vetores (embeddings).\n",
        "4. Armazenar: Salvar tudo em um banco de dados vetorial (FAISS) que funciona em memória."
      ],
      "metadata": {
        "id": "uZdVLKDiQatP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "print(\"Carregando o documento da web...\")\n",
        "# 1. Carregar: Vamos usar um artigo sobre o LangChain\n",
        "loader = WebBaseLoader(\n",
        "    \"https://python.langchain.com/docs/expression_language/\"\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(\"Criando o Vector Store (FAISS)...\")\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "print(\"\\n--- Teste Rápido do Retriever ---\")\n",
        "test_docs = retriever.invoke(\"O que é LCEL?\")\n",
        "print(f\"Encontrados {len(test_docs)} documentos relevantes para 'LCEL'.\")\n",
        "print(\"Retriever pronto!\")"
      ],
      "metadata": {
        "id": "eg4OqGpmPvDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e8b9c6-ffae-420c-b7c9-907cd547bb8b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando o documento da web...\n",
            "Criando o Vector Store (FAISS)...\n",
            "\n",
            "--- Teste Rápido do Retriever ---\n",
            "Encontrados 4 documentos relevantes para 'LCEL'.\n",
            "Retriever pronto!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo o Estado do Grafo (A \"Memória\")\n"
      ],
      "metadata": {
        "id": "cU9LzjNQRTbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class GraphState(TypedDict, total=False):\n",
        "    \"\"\"\n",
        "    Representa o estado do nosso agente.\n",
        "    \"\"\"\n",
        "    question: str       # A pergunta original do usuário\n",
        "    documents: List[Document] # Documentos recuperados pelo RAG\n",
        "    context: str        # O contexto formatado para o LLM\n",
        "    generation: str     # A resposta final do LLM\n",
        "    error: str          # Mensagem de erro, se houver"
      ],
      "metadata": {
        "id": "7UJ_5mlvRSOV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo os Nós (Nodes)"
      ],
      "metadata": {
        "id": "8U0UF7HXRyhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# 1. O Nó de Recuperação\n",
        "def retrieve_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Busca documentos no vector store com base na pergunta.\n",
        "    \"\"\"\n",
        "    print(\"--- 1. EXECUTANDO NODE: retrieve_node ---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = retriever.invoke(question)\n",
        "    print(f\"Documentos recuperados: {len(documents)}\")\n",
        "\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "def augment_prompt_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Prepara o contexto a partir dos documentos recuperados.\n",
        "    \"\"\"\n",
        "    print(\"--- 2. EXECUTANDO NODE: augment_prompt_node ---\")\n",
        "\n",
        "    # Junta o conteúdo dos documentos\n",
        "    context = \"\\n\\n---\\n\\n\".join(\n",
        "        [doc.page_content for doc in state[\"documents\"]]\n",
        "    )\n",
        "    # print(f\"Context: {context}\")\n",
        "    return {\"context\": context}\n",
        "\n",
        "# 3. O  Nó de Geração de Resposta\n",
        "def generate_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Gera uma resposta usando o LLM (com o contexto).\n",
        "    \"\"\"\n",
        "    print(\"--- 3. EXECUTANDO NODE: generate_node (RAG) ---\")\n",
        "\n",
        "    # Template do prompt RAG\n",
        "    template = \"\"\"Você é um assistente de IA. Use o contexto fornecido\n",
        "    para responder à pergunta do usuário da melhor forma possível.\n",
        "\n",
        "    Contexto:\n",
        "    {context}\n",
        "\n",
        "    Pergunta:\n",
        "    {question}\n",
        "    \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    # Modelo LLM (Gemini)\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "    # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "\n",
        "    # Cria a \"chain\" (Prompt + LLM + Parser)\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Invoca a chain\n",
        "    generation = chain.invoke({\n",
        "        \"context\": state[\"context\"],\n",
        "        \"question\": state[\"question\"]\n",
        "    })\n",
        "\n",
        "    return {\"generation\": generation}\n",
        "\n",
        "# 4. O Nó de Fallback/Erro\n",
        "def error_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Gera uma resposta padrão quando nenhum documento é encontrado.\n",
        "    \"\"\"\n",
        "    print(\"--- 3b. EXECUTANDO NODE: error_node (Fallback) ---\")\n",
        "\n",
        "    error_message = \"Desculpe, não consegui encontrar nenhuma informação sobre isso em minha base de dados.\"\n",
        "\n",
        "    return {\"generation\": error_message}"
      ],
      "metadata": {
        "id": "Mg_qBhxaRyCe"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo as \"Decisões\" (Arestas Condicionais)"
      ],
      "metadata": {
        "id": "HpRhStrRSE1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def should_augment(state: GraphState) -> str:\n",
        "    \"\"\"\n",
        "    Decide para qual node ir após a recuperação.\n",
        "    \"\"\"\n",
        "    print(\"--- DECISÃO: should_augment ---\")\n",
        "\n",
        "    if state[\"documents\"]:\n",
        "        print(\"Decisão: Documentos encontrados. Seguindo para 'augment_prompt'\")\n",
        "        return \"augment\"\n",
        "    else:\n",
        "        print(\"Decisão: Nenhum documento encontrado. Seguindo para 'error'\")\n",
        "        return \"fallback\""
      ],
      "metadata": {
        "id": "bPXu5tWGSDuL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Montando o Grafo\n",
        "\n",
        "1. Instanciar o `StateGraph`.\n",
        "2. Adicionar os Nós.\n",
        "3. Definir o ponto de partida (`set_entry_point`).\n",
        "4. Adicionar as conexões (`add_edge`) e as decisões (`add_conditional_edge`).\n",
        "5. Compilar o grafo."
      ],
      "metadata": {
        "id": "JHuXRBIiSNsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "print(\"Montando o grafo...\")\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"retrieve\", retrieve_node)\n",
        "workflow.add_node(\"augment_prompt\", augment_prompt_node)\n",
        "workflow.add_node(\"generate\", generate_node)\n",
        "workflow.add_node(\"error\", error_node)\n",
        "\n",
        "\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "# A aresta condicional\n",
        "workflow.add_conditional_edges(\n",
        "    # O nó de onde a decisão sai (como você apontou)\n",
        "    source=\"retrieve\",\n",
        "\n",
        "    # A função que decide o caminho (retorna \"augment\" ou \"fallback\")\n",
        "    path=should_augment,\n",
        "\n",
        "    # O mapa de destinos\n",
        "    path_map={\n",
        "        \"augment\": \"augment_prompt\",\n",
        "        \"fallback\": \"error\"\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "workflow.add_edge(\"augment_prompt\", \"generate\")\n",
        "\n",
        "workflow.add_edge(\"generate\", END)\n",
        "workflow.add_edge(\"error\", END)\n",
        "\n",
        "\n",
        "# Compilar o grafo!\n",
        "app = workflow.compile()\n",
        "print(\"Grafo compilado com sucesso!\")"
      ],
      "metadata": {
        "id": "WfmLpkj2SMog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15412ef3-e174-4b42-857e-5a8230f532d6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Montando o grafo...\n",
            "Grafo compilado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executando o Agente RAG"
      ],
      "metadata": {
        "id": "c8NBso_MUp6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- INICIANDO TESTE 1 (CAMINHO FELIZ) ---\")\n",
        "question = \"O que é LangChain?\"\n",
        "\n",
        "final_state = app.invoke({\"question\": question})\n",
        "\n",
        "print(\"\\n--- RESPOSTA FINAL (Teste 1) ---\")\n",
        "print(final_state[\"generation\"])"
      ],
      "metadata": {
        "id": "PxevMj2ySsMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76132825-50ed-427e-b528-c1689b2ade49"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- INICIANDO TESTE 1 (CAMINHO FELIZ) ---\n",
            "--- 1. EXECUTANDO NODE: retrieve_node ---\n",
            "Documentos recuperados: 4\n",
            "--- DECISÃO: should_augment ---\n",
            "Decisão: Documentos encontrados. Seguindo para 'augment_prompt'\n",
            "--- 2. EXECUTANDO NODE: augment_prompt_node ---\n",
            "--- 3. EXECUTANDO NODE: generate_node (RAG) ---\n",
            "\n",
            "--- RESPOSTA FINAL (Teste 1) ---\n",
            "Com base no contexto fornecido, LangChain é:\n",
            "\n",
            "Uma ferramenta que oferece a maneira mais fácil de começar a construir **agentes e aplicações alimentadas por LLMs** (Large Language Models).\n",
            "\n",
            "**Pontos-chave sobre LangChain:**\n",
            "\n",
            "*   Permite conectar-se a provedores de LLMs como OpenAI, Anthropic e Google com menos de 10 linhas de código.\n",
            "*   Fornece uma **arquitetura de agente pré-construída** e integrações de modelo para incorporar LLMs de forma rápida e contínua.\n",
            "*   É recomendado para quem deseja construir **agentes e aplicações autônomas rapidamente**.\n",
            "*   Oferece uma **interface de modelo padrão** que padroniza a interação com diferentes provedores, permitindo a troca sem problemas.\n",
            "*   Sua abstração de agente é **fácil de usar** (possibilitando a criação de um agente simples com poucas linhas de código) e **altamente flexível**.\n",
            "*   Os agentes LangChain são construídos sobre o **LangGraph**, aproveitando recursos como execução durável, streaming, \"human-in-the-loop\" e persistência, embora você não precise conhecer LangGraph para o uso básico de agentes LangChain.\n",
            "*   Permite a **depuração com LangSmith** para obter visibilidade profunda do comportamento de agentes complexos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n--- INICIANDO TESTE 2 (CAMINHO DE FALLBACK) ---\")\n",
        "question_fallback = \"Qual é a capital da Eslovênia?\"\n",
        "\n",
        "\n",
        "final_state_fallback = app.invoke({\"question\": question_fallback})\n",
        "\n",
        "print(\"\\n--- RESPOSTA FINAL (Teste 2) ---\")\n",
        "print(final_state_fallback[\"generation\"])"
      ],
      "metadata": {
        "id": "2ojwPyYaUvZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9e266e-fa60-43cf-863b-75a088885ebd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- INICIANDO TESTE 2 (CAMINHO DE FALLBACK) ---\n",
            "--- 1. EXECUTANDO NODE: retrieve_node ---\n",
            "Documentos recuperados: 4\n",
            "--- DECISÃO: should_augment ---\n",
            "Decisão: Documentos encontrados. Seguindo para 'augment_prompt'\n",
            "--- 2. EXECUTANDO NODE: augment_prompt_node ---\n",
            "--- 3. EXECUTANDO NODE: generate_node (RAG) ---\n",
            "\n",
            "--- RESPOSTA FINAL (Teste 2) ---\n",
            "O contexto fornecido é sobre a criação de um agente de IA usando LangChain e não contém informações sobre a capital da Eslovênia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZPuSIOu4YOBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}